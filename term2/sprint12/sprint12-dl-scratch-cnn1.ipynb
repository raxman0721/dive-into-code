{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】1次元畳み込み後の出力サイズの計算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】小さな配列での1次元畳み込み層の実験\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】学習・推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:59:45.900237Z",
     "start_time": "2019-07-16T01:59:42.627770Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#インポート\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pyprind\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:23.141341Z",
     "start_time": "2019-07-16T01:57:23.105078Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"\n",
    "    1次元畳み込み層クラス\n",
    "    2次元にも対応可能\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    FN : int\n",
    "      出力チャンネル数（フィルターの個数）\n",
    "    FH : int\n",
    "      フィルターの高さ\n",
    "    FW : int\n",
    "      フィルターの幅\n",
    "    S : int (default: 1)\n",
    "      ストライド\n",
    "    P : int (default: 0)\n",
    "      パディング\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'conv'\n",
    "      layerの種類\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 FN=3,\n",
    "                 FH=3,\n",
    "                 FW=3,\n",
    "                 P=0,\n",
    "                 S=1):\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する           \n",
    "        self.FH = FH\n",
    "        self.FW = FW\n",
    "        self.FN = FN\n",
    "        self.P = P\n",
    "        self.S = S\n",
    "        self.group = 'conv'\n",
    "        \n",
    "    def initialize(self, input_dim,summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        W,B,initializer,optimizerを初期化するメソッド\n",
    "        \n",
    "        Parameters\n",
    "        ----------        \n",
    "        input_dim :次の形のtuple, (入力チャンネル,高さ,横幅)\n",
    "            入力サイズ\n",
    "        summary: bool\n",
    "            Trueにするとshapeを出力\n",
    "        initializer: class \n",
    "            initializerのクラス\n",
    "        optimizer: \n",
    "            optimizerのクラス\n",
    "        sigma:float\n",
    "            Simpleinitializerを選んだ時のパラメータ\n",
    "            \n",
    "        Return\n",
    "        ----------\n",
    "        out_dim:次の形のtuple, (出力チャンネル,OH,OW)\n",
    "        \"\"\"\n",
    "        #出力サイズを計算する。\n",
    "        C, H, W = input_dim\n",
    "        OH = out_calc(H, self.P, self.FH, self.S)\n",
    "        OW = out_calc(W, self.P, self.FW, self.S)\n",
    "        out_dim = (self.FN, OH, OW)\n",
    "        \n",
    "        #初期値を設定する。\n",
    "        initializer = Initializer(init_type, C * self.FH * self.FW, sigma)\n",
    "        self.W = initializer.W(self.FN, C, self.FH, self.FW)\n",
    "        self.B = initializer.B(self.FN)\n",
    "        if summary:\n",
    "            print(self.group,'layer shape={}, param={}'.format(out_dim, self.FH*self.FW*C*self.FN+self.FN))\n",
    "                \n",
    "        #optimizerを設定する。\n",
    "        self.optimizer = optimizer(lr)\n",
    "        return out_dim \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (N,C,高さ,横幅)\n",
    "            入力値\n",
    "        W: 次の形のndarray, shape (FN,C,FH,FW)\n",
    "            重み\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        out : 次の形のndarray, shape (N、FN,OH, OW)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        X_copy = X.copy()\n",
    "        self.X_shape = X_copy.shape #バックワードで使うのでshapeを保存しておく。\n",
    "        self.X_out = im2patch(X_copy, self.FH, self.FW, self.S, self.P)  \n",
    "        # X_out.shape(N, C, OH(dAをかける), OW(dAをかける), input_h(Wをかける), input_w(Wをかける))\n",
    "        out = np.tensordot(self.X_out, self.W, axes=[[1, 4, 5], [1, 2, 3]]) \n",
    "        out = out.transpose([0, 3, 1, 2])  # (N、FN,OH, OW) の形に直す\n",
    "        out += self.B[np.newaxis, :, np.newaxis, np.newaxis] #FN分だけBをブロードキャストして足す。\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (N、FN,OH, OW)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dW: 次の形のndarray, shape (FN,C,FH,FW)\n",
    "            前に流すWの勾配\n",
    "        dB : 次の形のndarray, shape (N,FN)\n",
    "            前に流すBの勾配\n",
    "        dX : 次の形のndarray, shape (N,C,高さ,横幅)\n",
    "            前に流すWの勾配\n",
    "\n",
    "        \"\"\"\n",
    "        #勾配を計算\n",
    "        self.dW = np.tensordot(dA, self.X_out, axes=[[0, 2, 3], [0, 2,3]])  #N,OH,OWの軸でテンソルドットする。\n",
    "        self.dB = np.sum(dA, axis=(-1, -2))  # (N、FN)を残してsum\n",
    "        # 出力を計算\n",
    "        out = np.tensordot(dA, self.W, axes=[1, 0])#FNの軸でdot積\n",
    "        out = out.transpose(0, 3, 1,2, 4, 5)\n",
    "        # out.shape(N,C,OH, OW, FH, FW)\n",
    "        dX = patch2im(out, self.X_shape, self.S, self.P)\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:27.689208Z",
     "start_time": "2019-07-16T01:57:27.684385Z"
    }
   },
   "outputs": [],
   "source": [
    "def out_calc(n_in, P=0, FS=3, S=1):\n",
    "    \"\"\"\n",
    "    出力サイズの計算。1次元。\n",
    "    n_in: int\n",
    "        特徴量の数\n",
    "    p: int(0)\n",
    "        パディングの数\n",
    "    FS:int(3)\n",
    "        フィルタのサイズ\n",
    "    s: int(1)\n",
    "        ストライドのサイズ        \n",
    "    \"\"\"\n",
    "    return int(((n_in + 2 * P - FS) / S) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:31.080343Z",
     "start_time": "2019-07-16T01:57:31.063958Z"
    }
   },
   "outputs": [],
   "source": [
    "def im2patch(input_data, FH, FW, S=1, P=0):\n",
    "    \"\"\"\n",
    "    ストライドした後の配列を出してくれる関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\n",
    "    FH : フィルターの高さ\n",
    "    FW : フィルターの幅\n",
    "    S : ストライド\n",
    "    P : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    patch : shape(データ数, チャンネル, OH(dAをかける), OW(dAをかける), FH(Wをかける), FW(Wをかける)) \n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    OH = out_calc(H, P, FH, S)\n",
    "    OW = out_calc(W, P, FW, S)\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (P, P), (P, P)],'constant')\n",
    "    patch = np.empty((N, C, OH, OW, FH, FW))\n",
    "    #出力サイズ(OH,OW)の軸を固定してフィルター(FH,HW)を掛けた部分を代入していく\n",
    "    for y in range(OH):\n",
    "        IH = y * S\n",
    "        IH_max = IH + FH\n",
    "        for x in range(OW):\n",
    "            IW = x * S\n",
    "            IW_max = IW + FW\n",
    "            patch[:, :, y, x, :, :] = img[:, :, IH:IH_max, IW:IW_max]\n",
    "    return patch\n",
    "\n",
    "\n",
    "def patch2im(input_data, input_shape, S=1, P=0):\n",
    "    \"\"\"\n",
    "    ストライドした後の配列を出してくれる関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : (データ数,チャンネル数, OH, OW,FH, FW)の6次元配列からなる入力データ\n",
    "    input_shape:(データ数,チャンネル数、高さ、横幅)\n",
    "    S : ストライド\n",
    "    P : パディング\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    back\n",
    "    out : shape(データ数, チャンネル, IH, IW)\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    _, _,  OH, OW, FH, FW = input_data.shape\n",
    "    im = np.empty((N, C, H + 2 * P, W + 2 * P ))\n",
    "    # im2patchと逆のことをする。\n",
    "    for y in range(OH):\n",
    "        IH = y * S\n",
    "        IH_max = IH + FH \n",
    "        for x in range(OW):\n",
    "            IW = x * S\n",
    "            IW_max = IW + FW\n",
    "            im[:, :, IH:IH_max, IW:IW_max] += input_data[:, :, y, x, :, :]\n",
    "\n",
    "    return im[:, :, P:P + H, P:P + W]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:31.981895Z",
     "start_time": "2019-07-16T01:57:31.973868Z"
    }
   },
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10):\n",
    "        self.batch_size = batch_size\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:33.013203Z",
     "start_time": "2019-07-16T01:57:33.001345Z"
    }
   },
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数pre_nodesからnodesへの全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    nodes : int\n",
    "      層のノード数\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'FC'\n",
    "      layerの種類\n",
    "    \"\"\"\n",
    "    def __init__(self, nodes):\n",
    "        self.nodes = nodes\n",
    "        self.group = 'FC'\n",
    "        \n",
    "    def initialize(self, pre_nodes, summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        重み、バイアスを初期化して出力数を渡してあげる\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_dim :次の形のtuple, (入力チャンネル,高さ,横幅)\n",
    "            入力サイズ\n",
    "        initializer: class\n",
    "            initializerのクラス\n",
    "        optimizer: class\n",
    "            optimizerのクラス\n",
    "        lr : float(1e-2)\n",
    "            optimizerに渡す学習率\n",
    "        sigma : float(1e-2)\n",
    "            Simpleinitializerを選んだ時のパラメータ\n",
    "        \"\"\"\n",
    "        \n",
    "        #初期値を設定する。\n",
    "        initializer = Initializer(init_type, pre_nodes, sigma)\n",
    "        self.W = initializer.W(pre_nodes, self.nodes)\n",
    "        self.B = initializer.B(self.nodes)\n",
    "        \n",
    "        if summary:\n",
    "            print(self.group,'layer shape={}, param={}'.format(self.W.shape,pre_nodes*self.nodes+self.nodes))\n",
    "\n",
    "        #optimizerを設定する。\n",
    "        self.optimizer = optimizer(lr)\n",
    "        \n",
    "        return self.nodes\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, pre_nodes)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, nodes)\n",
    "            出力\n",
    "        \"\"\"        \n",
    "        self.Z = X.copy()\n",
    "        A = X @ self.W + self.B\n",
    "        return A\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, nodes)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, pre_nodes)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        self.dB = dA\n",
    "        self.dW = self.Z.T @ dA\n",
    "        dZ = dA @ self.W.T\n",
    "        # 重みを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:52.092883Z",
     "start_time": "2019-07-16T01:57:52.081324Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        layer.W -= self.lr * (layer.dW / layer.dB.shape[0])\n",
    "        layer.B -= self.lr * np.mean(layer.dB, axis=0)\n",
    "        return layer\n",
    "    \n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGrad\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_w = 0 #最初は0\n",
    "        self.H_b = 0\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        dW_mean = layer.dW / layer.dB.shape[0] #行列\n",
    "        dB_mean = np.mean(layer.dB, axis=0)\n",
    "        #重みの更新\n",
    "        self.H_w += dW_mean * dW_mean # Hを更新\n",
    "        layer.W -= self.lr * dW_mean / (np.sqrt(self.H_w) + 1e-7)\n",
    "        \n",
    "        self.H_b +=  dB_mean * dB_mean        \n",
    "        layer.B -= self.lr * dB_mean / (np.sqrt(self.H_b) + 1e-7)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:53.462552Z",
     "start_time": "2019-07-16T01:57:53.439798Z"
    }
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    　シグモイド関数の活性化関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "        self.group = 'activation'\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        self.Z = 1 / (1 + np.exp(-A))\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列  shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        dA = dZ * (1 - self.Z) * self.Z\n",
    "        return dA\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "        self.group = 'activation'\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列 元のshapeを保持\n",
    "        \"\"\"\n",
    "        self.Z = np.tanh(A)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列 shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        dA = dZ * (1 - self.Z ** 2)\n",
    "        return dA\n",
    "    \n",
    "class Relu:\n",
    "    \"\"\"\n",
    "    ReLU関数の活性化関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.group = 'activation'\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        self.mask = A <= 0\n",
    "        Z = A.copy()\n",
    "        Z[self.mask] = 0\n",
    "        return  Z\n",
    "\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列shapeはどんな形でも大丈夫\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　元のshapeを保持\n",
    "        \"\"\"\n",
    "        dZ[self.mask] = 0\n",
    "        \n",
    "        return dZ\n",
    "    \n",
    "class Softmax:\n",
    "    \"\"\"\n",
    "    ソフトマックス関数の活性化関数\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.Z = None\n",
    "        self.entropy = None # バッチ単位でのエントロピー\n",
    "        self.group = 'activation'\n",
    "        \n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロパゲーションのときのメソッド\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 全結合後の行列 shape(batch_size, pre_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 活性化後の行列　shape(batch_size,  pre_nodes)\n",
    "        \"\"\"\n",
    "        c = np.max(A, axis=1,keepdims=True)\n",
    "        self.Z = np.exp(A-c) / np.sum(np.exp(A-c), axis=1).reshape(-1, 1)\n",
    "        return self.Z\n",
    "    \n",
    "    def backward(self, Y):\n",
    "        \"\"\"\n",
    "        バックワードと交差エントロピーを計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        dZ : 全結合後の行列 shape(batch_size, pre_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        dA : 活性化後の行列　shape(batch_size,  pre_nodes)\n",
    "        \"\"\"\n",
    "        entropy = -np.sum(Y * np.log(self.Z + 1e-5), axis=1) #サンプル毎のエントロピー(batch_size,)\n",
    "        self.entropy = entropy.sum() / len(entropy)  # スカラー\n",
    "        #勾配はこっち\n",
    "        dA = self.Z - Y\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:54.593613Z",
     "start_time": "2019-07-16T01:57:54.584432Z"
    }
   },
   "outputs": [],
   "source": [
    "class Initializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期値設定\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, init_type, pre_nodes, sigma):\n",
    "        if init_type == 'simple':\n",
    "            self.sigma = sigma\n",
    "        elif init_type == 'Xavier':\n",
    "            self.sigma = 1 / np.sqrt(pre_nodes)\n",
    "        elif init_type == 'He':\n",
    "            self.sigma = np.sqrt(2 / pre_nodes)\n",
    "\n",
    "    def W(self,*args):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : int\n",
    "          ノード数や、チャンネル数等必要なサイズを入力\n",
    "        Returns\n",
    "        ----------\n",
    "        W :次の形のndarray, shape (args)\n",
    "            重み\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.standard_normal(size=args)\n",
    "        return W\n",
    "    def B(self, *args):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        args : int\n",
    "          ノード数等を入力。入力した形の必要なサイズを入力\n",
    "        Returns\n",
    "        ----------\n",
    "        B :次の形のndarray, shape (args)\n",
    "            バイアス\n",
    "        \"\"\"\n",
    "        B = self.sigma * np.random.standard_normal(size=args)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:57:55.140143Z",
     "start_time": "2019-07-16T01:57:55.130917Z"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    \"\"\"\n",
    "    平滑化するクラス\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    group : 'flatten'\n",
    "      layerの種類\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化    \n",
    "        self.group = 'flatten'\n",
    "        \n",
    "    def initialize(self, input_dim,summary, *args, **kargs):\n",
    "        \"\"\"\n",
    "        出力サイズを出力するメソッド\n",
    "        [*args, **kargs]はダミー    \n",
    "        Return\n",
    "        ----------\n",
    "        out_dim:次の形のtuple, (出力チャンネル,OH,OW)\n",
    "        \"\"\"\n",
    "        #出力サイズを計算する。\n",
    "        C, H, W = input_dim\n",
    "        if summary:\n",
    "            print(self.group,'layer shape=', (C*H*W))\n",
    "\n",
    "        return C*H*W\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (N,C,高さ,横幅)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (N、C*高さ*横幅)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.X_shape = X.shape #バックワードで使用\n",
    "        out = X.reshape(len(X), -1) #サンプルサイズだけ残してflatへ\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dout: 次の形のndarray, shape(N, n_nodes)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (N、C,高さ,横幅)\n",
    "            出力\n",
    "        \"\"\"\n",
    "                \n",
    "        dout = dout.reshape(self.X_shape)\n",
    "        return dout\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:58:12.601535Z",
     "start_time": "2019-07-16T01:58:12.574731Z"
    }
   },
   "outputs": [],
   "source": [
    "class Scratch1dCNNClassifier:\n",
    "    \"\"\"\n",
    "    ディープなニューラルネットワーク分類器\n",
    "    層を増やすことが出来る。\n",
    "    バッチをランダムで抽出する。\n",
    "    エポック毎にバッチを取り直すことも可能。\n",
    "    2Dにも対応可能\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int　(30)\n",
    "        バッチサイズ\n",
    "    n_epoch : int (100)\n",
    "        エポック数\n",
    "    e_threshold : float(1e-2)\n",
    "        エポック途中終了の為のエントロピーの閾値\n",
    "    n_iter : int(1000)\n",
    "        1エポック辺りのイテレーション数\n",
    "    repeat_batch_process : bool(True)\n",
    "        Trueの場合１エポック毎にバッチをランダムに取り直す。\n",
    "    restore_extraction:bool(True)\n",
    "        学習するバッチをランダム抽出する際に復元か、非復元か選ぶ。基本は復元(ブートストラップ)\n",
    "    seed : int(0)\n",
    "        ランダムシード\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    entropy : shape(n_epoch, n_iter)\n",
    "        1バッチごとのエントロピー\n",
    "    layers : list\n",
    "        layerのリスト\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size=30,\n",
    "        n_epochs=100,\n",
    "        e_threshold=1e-2,\n",
    "        n_iter=1000,\n",
    "        repeat_batch_process=True,\n",
    "        restore_extraction=True,\n",
    "        seed=0\n",
    "    ):\n",
    "        self._n_iter = n_iter\n",
    "        self._repeat_batch_process = repeat_batch_process\n",
    "        self._restore_extraction = restore_extraction\n",
    "        self._batch_size = batch_size\n",
    "        self._n_epochs = n_epochs\n",
    "        self._e_threshold = e_threshold  # 誤差の閾値        \n",
    "        self.entropy = None\n",
    "        self.epoch_entropy_mean = None\n",
    "                            \n",
    "    def sequential(self,*layers):\n",
    "        \"\"\"\n",
    "        layerをつなげるメソッド。\n",
    "        \"\"\"\n",
    "        self.layers = []        \n",
    "        for layer in layers:\n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def initialize(self, input_dim, summary, init_type, optimizer, sigma=1e-2, lr=1e-2):\n",
    "        \"\"\"\n",
    "        それぞれのlayerの初期化メソッド\n",
    "        活性化層以外の層のinitializeメソッドを使う。\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if layer.group != 'activation':\n",
    "                input_dim = layer.initialize(input_dim,summary, init_type, optimizer, sigma=sigma, lr=lr)\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        NNを学習する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, チャンネル数,高さ,幅)\n",
    "            学習用データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, クラス)\n",
    "            学習用データの正解値(one_hot_vectaされた後のもの)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # リピートしない場合はここでミニバッチ化\n",
    "        if self._repeat_batch_process == False:\n",
    "            train_batch = GetMiniBatch(X, y,\n",
    "                                       batch_size=self._batch_size) \n",
    "        # バッチをランダム取得するためのindexを取得\n",
    "        batch_index = np.random.choice(int(len(X) / self._batch_size),\n",
    "                                       self._n_iter,\n",
    "                                       replace=self._restore_extraction)  # n_iterのindex\n",
    "        self.entropy = np.zeros(\n",
    "            (self._n_epochs, self._n_iter))  # エポック数×イテレーション数のエントロピーの入れ物\n",
    "\n",
    "        # 学習開始\n",
    "        for epoch in tqdm(range(self._n_epochs)):\n",
    "            if self._repeat_batch_process:\n",
    "                train_batch = GetMiniBatch(\n",
    "                    X, y, batch_size=self._batch_size)  # バッチに分ける。\n",
    "            batch_entropy = np.zeros(self._n_iter)  # 1エポック内でのエントロピーを格納。\n",
    "            for i, index in enumerate(batch_index):\n",
    "                X_batch, y_batch = train_batch[index][0].copy(\n",
    "                ), train_batch[index][1].copy()\n",
    "                # フォワードプロパゲーション\n",
    "                for layer_index in range(len(self.layers)):\n",
    "                    X_batch = self.layers[layer_index].forward(X_batch)\n",
    "                # バックプロパゲーション\n",
    "                for layer_index in range(1, len(self.layers) + 1):\n",
    "                    y_batch = self.layers[-layer_index].backward(y_batch)\n",
    "\n",
    "                    # 誤差を格納\n",
    "                batch_entropy[i] = self.layers[-1].entropy\n",
    "                if i % 100 == 0: #100 バッチごとに状況を報告\n",
    "                    print('{}回目のbatch_entropy= {:.5f}'.format(i+epoch*self._n_iter,batch_entropy[i]))\n",
    "\n",
    "            self.entropy[epoch, :] = batch_entropy  # バッチごとのエントロピーを格納\n",
    "            if batch_entropy.mean() < self._e_threshold:  # 誤差が閾値以下になったらエポック終了\n",
    "                print('entropyが{:.3f}より低いよ！'.format(self._e_threshold))\n",
    "                break\n",
    "                \n",
    "        #最後までエポックが回らなかった時に後ろの0を消去\n",
    "        self.entropy = self.entropy[self.entropy != 0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        NNで予測する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, チャンネル数,高さ,幅)\n",
    "            学習用データの特徴量        \n",
    "        \"\"\"\n",
    "\n",
    "        # フォワードプロパゲーション\n",
    "        out = X.copy()\n",
    "        for layer in range(len(self.layers)):\n",
    "            out = self.layers[layer].forward(out)\n",
    "\n",
    "        return np.argmax(out, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:59:49.457595Z",
     "start_time": "2019-07-16T01:59:48.337992Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:59:51.815982Z",
     "start_time": "2019-07-16T01:59:51.799724Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, 1)\n",
    "X_test = np.expand_dims(X_test, 1)\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:59:52.627872Z",
     "start_time": "2019-07-16T01:59:52.621720Z"
    }
   },
   "outputs": [],
   "source": [
    "cnn1d = Scratch1dCNNClassifier(batch_size=100,\n",
    "                                n_epochs=5,\n",
    "                                n_iter=500,\n",
    "                                repeat_batch_process=False,\n",
    "                                restore_extraction=False,\n",
    "                                seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T01:59:56.782554Z",
     "start_time": "2019-07-16T01:59:56.730038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv layer shape=(6, 30, 30), param=60\n",
      "flatten layer shape= 5400\n",
      "FC layer shape=(5400, 128), param=691328\n",
      "FC layer shape=(128, 50), param=6450\n",
      "FC layer shape=(50, 10), param=510\n"
     ]
    }
   ],
   "source": [
    "cnn1d.sequential(\n",
    "    Conv1d(FN=6,FH=3, FW=3,  P=2, S=1), \n",
    "    Relu(),\n",
    "    Flatten(), \n",
    "    FC(128),\n",
    "    Relu(),\n",
    "    FC(50), \n",
    "    Relu(),\n",
    "    FC(10), \n",
    "    Softmax()\n",
    ")\n",
    "\n",
    "input_dim = (1, 28, 28)\n",
    "cnn1d.initialize(input_dim, summary=True, init_type='He', optimizer=AdaGrad, lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T02:05:39.914863Z",
     "start_time": "2019-07-16T02:00:23.432654Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70dd1e136004383b785736a230d08bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0回目のbatch_entropy= 2.57254\n",
      "100回目のbatch_entropy= 0.62507\n",
      "200回目のbatch_entropy= 0.42283\n",
      "300回目のbatch_entropy= 0.39267\n",
      "400回目のbatch_entropy= 0.38123\n",
      "500回目のbatch_entropy= 0.37014\n",
      "600回目のbatch_entropy= 0.30933\n",
      "700回目のbatch_entropy= 0.22057\n",
      "800回目のbatch_entropy= 0.24994\n",
      "900回目のbatch_entropy= 0.30004\n",
      "1000回目のbatch_entropy= 0.30345\n",
      "1100回目のbatch_entropy= 0.24930\n",
      "1200回目のbatch_entropy= 0.17263\n",
      "1300回目のbatch_entropy= 0.19195\n",
      "1400回目のbatch_entropy= 0.25865\n",
      "1500回目のbatch_entropy= 0.27347\n",
      "1600回目のbatch_entropy= 0.22414\n",
      "1700回目のbatch_entropy= 0.14656\n",
      "1800回目のbatch_entropy= 0.15867\n",
      "1900回目のbatch_entropy= 0.23502\n",
      "2000回目のbatch_entropy= 0.25301\n",
      "2100回目のbatch_entropy= 0.20447\n",
      "2200回目のbatch_entropy= 0.12859\n",
      "2300回目のbatch_entropy= 0.13901\n",
      "2400回目のbatch_entropy= 0.21894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn1d.fit(X_train, y_train_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-16T02:05:46.295895Z",
     "start_time": "2019-07-16T02:05:40.010155Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9561"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cnn1d.predict(X_test) == y_test).sum() / len(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
